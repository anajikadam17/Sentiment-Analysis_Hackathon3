{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import emoji\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "from sklearn.metrics import roc_auc_score ,mean_squared_error,accuracy_score,classification_report,confusion_matrix,roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.externals import joblib\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the data for mini-challenges\n",
    "df = pd.read_csv('data/train.csv')\n",
    "# df = df.reindex(np.random.permutation(df.index))\n",
    "df = df[['tweet', 'sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 7273 entries, 0 to 7273\n",
      "Data columns (total 2 columns):\n",
      "tweet        7273 non-null object\n",
      "sentiment    7273 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 170.5+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#sxswnui #sxsw #apple defining language of touch with different dialects becoming smaller</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Learning ab Google doodles! All doodles should be light, funny &amp;amp; innovative, with exceptions for significant occasions. #GoogleDoodle #sxsw</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>one of the most in-your-face ex. of stealing the show in yrs RT @mention &amp;quot;At #SXSW, Apple schools the mkt experts&amp;quot;  {link}</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This iPhone #SXSW app would b pretty awesome if it didn't crash every 10mins during extended browsing. #Fuckit #Illmakeitwork</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Line outside the Apple store in Austin waiting for the new iPad #SXSW  {link}</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             tweet  \\\n",
       "0  #sxswnui #sxsw #apple defining language of touch with different dialects becoming smaller                                                         \n",
       "1  Learning ab Google doodles! All doodles should be light, funny &amp; innovative, with exceptions for significant occasions. #GoogleDoodle #sxsw   \n",
       "2  one of the most in-your-face ex. of stealing the show in yrs RT @mention &quot;At #SXSW, Apple schools the mkt experts&quot;  {link}              \n",
       "3  This iPhone #SXSW app would b pretty awesome if it didn't crash every 10mins during extended browsing. #Fuckit #Illmakeitwork                     \n",
       "4  Line outside the Apple store in Austin waiting for the new iPad #SXSW  {link}                                                                     \n",
       "\n",
       "   sentiment  \n",
       "0  1          \n",
       "1  1          \n",
       "2  2          \n",
       "3  0          \n",
       "4  1          "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCounts(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def count_regex(self, pattern, tweet):\n",
    "        return len(re.findall(pattern, tweet))\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        # fit method is used when specific operations need to be done on the train data, but not on the test data\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        count_words = X.apply(lambda x: self.count_regex(r'\\w+', str(x))) \n",
    "        count_mentions = X.apply(lambda x: self.count_regex(r'@\\w+', str(x)))\n",
    "        count_hashtags = X.apply(lambda x: self.count_regex(r'#\\w+', str(x)))\n",
    "        count_capital_words = X.apply(lambda x: self.count_regex(r'\\b[A-Z]{2,}\\b', str(x)))\n",
    "        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\\?', str(x)))\n",
    "        count_urls = X.apply(lambda x: self.count_regex(r'http.?://[^\\s]+[\\s]?', str(x)))\n",
    "        # We will replace the emoji symbols with a description, which makes using a regex for counting easier\n",
    "        # Moreover, it will result in having more words in the tweet\n",
    "        count_emojis = X.apply(lambda x: emoji.demojize(str(x))).apply(lambda x: self.count_regex(r':[a-z_&]+:', str(x)))\n",
    "        \n",
    "        df = pd.DataFrame({'count_words': count_words\n",
    "                           , 'count_mentions': count_mentions\n",
    "                           , 'count_hashtags': count_hashtags\n",
    "                           , 'count_capital_words': count_capital_words\n",
    "                           , 'count_excl_quest_marks': count_excl_quest_marks\n",
    "                           , 'count_urls': count_urls\n",
    "                           , 'count_emojis': count_emojis\n",
    "                          })\n",
    "        \n",
    "        return df\n",
    "tc = TextCounts()\n",
    "df_eda = tc.fit_transform(df.tweet)\n",
    "df_eda['sentiment'] = df.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_words  count_mentions  count_hashtags  count_capital_words  \\\n",
       "0  12           0               3               0                     \n",
       "1  19           0               2               0                     \n",
       "2  26           1               1               2                     \n",
       "3  20           0               3               1                     \n",
       "4  14           0               1               1                     \n",
       "\n",
       "   count_excl_quest_marks  count_urls  count_emojis  sentiment  \n",
       "0  0                       0           0             1          \n",
       "1  1                       0           0             1          \n",
       "2  0                       0           0             2          \n",
       "3  0                       0           0             0          \n",
       "4  0                       0           0             1          "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eda.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7180    pack point show iphon fragment sxsw                                                           \n",
       "770     sxsw crowd austin swarm ipad launch link via sadli one                                        \n",
       "1721    ze frank project walk googl streetview street walk mani time revel pop sxsw                   \n",
       "4262    anyon know abl find video marissa mayer sxsw present googl                                    \n",
       "6493    ipad giveaway full forc person friend citi end sxsw win hint strong email list work well sxswi\n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', str(input_text))\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', str(input_text))\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', str(input_text))\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def stemming(self, input_text):\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)\n",
    "        return clean_X\n",
    "\n",
    "ct = CleanText()\n",
    "sr_clean = ct.fit_transform(df.tweet)\n",
    "sr_clean.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 records have no words left after text cleaning\n"
     ]
    }
   ],
   "source": [
    "empty_clean = sr_clean == ''\n",
    "# sr_clean[empty_clean].count()\n",
    "print('{} records have no words left after text cleaning'.format(sr_clean[empty_clean].count()))\n",
    "sr_clean.loc[empty_clean] = '[no_text]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    sxswnui sxsw appl defin languag touch differ dialect becom smaller                     \n",
       "1    learn ab googl doodl doodl light funni amp innov except signific occas googledoodl sxsw\n",
       "2    one face ex steal show yr rt quot sxsw appl school mkt expert quot link                \n",
       "3    iphon sxsw app would pretti awesom crash everi min extend brows fuckit illmakeitwork   \n",
       "4    line outsid appl store austin wait new ipad sxsw link                                  \n",
       "Name: tweet, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(sr_clean)\n",
    "word_freq = dict(zip(cv.get_feature_names(), np.asarray(bow.sum(axis=0)).ravel()))\n",
    "word_counter = collections.Counter(word_freq)\n",
    "word_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.barplot(x=\"word\", y=\"freq\", data=word_counter_df, palette=\"PuBuGn_d\", ax=ax)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count_words',\n",
       " 'count_mentions',\n",
       " 'count_hashtags',\n",
       " 'count_capital_words',\n",
       " 'count_excl_quest_marks',\n",
       " 'count_urls',\n",
       " 'count_emojis',\n",
       " 'sentiment',\n",
       " 'clean_text']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model = df_eda\n",
    "df_model['clean_text'] = sr_clean\n",
    "col = df_model.columns.tolist()\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count_words</th>\n",
       "      <th>count_mentions</th>\n",
       "      <th>count_hashtags</th>\n",
       "      <th>count_capital_words</th>\n",
       "      <th>count_excl_quest_marks</th>\n",
       "      <th>count_urls</th>\n",
       "      <th>count_emojis</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>sxswnui sxsw appl defin languag touch differ dialect becom smaller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>learn ab googl doodl doodl light funni amp innov except signific occas googledoodl sxsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>one face ex steal show yr rt quot sxsw appl school mkt expert quot link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>iphon sxsw app would pretti awesom crash everi min extend brows fuckit illmakeitwork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>line outsid appl store austin wait new ipad sxsw link</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count_words  count_mentions  count_hashtags  count_capital_words  \\\n",
       "0  12           0               3               0                     \n",
       "1  19           0               2               0                     \n",
       "2  26           1               1               2                     \n",
       "3  20           0               3               1                     \n",
       "4  14           0               1               1                     \n",
       "\n",
       "   count_excl_quest_marks  count_urls  count_emojis  sentiment  \\\n",
       "0  0                       0           0             1           \n",
       "1  1                       0           0             1           \n",
       "2  0                       0           0             2           \n",
       "3  0                       0           0             0           \n",
       "4  0                       0           0             1           \n",
       "\n",
       "                                                                                clean_text  \n",
       "0  sxswnui sxsw appl defin languag touch differ dialect becom smaller                       \n",
       "1  learn ab googl doodl doodl light funni amp innov except signific occas googledoodl sxsw  \n",
       "2  one face ex steal show yr rt quot sxsw appl school mkt expert quot link                  \n",
       "3  iphon sxsw app would pretti awesom crash everi min extend brows fuckit illmakeitwork     \n",
       "4  line outsid appl store austin wait new ipad sxsw link                                    "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "mnb = MultinomialNB()\n",
    "cl = SVC(kernel='rbf', gamma = 6, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {'Logistic Regression':LogisticRegression()\n",
    "          ,'Decision Tree':DecisionTreeClassifier()\n",
    "          ,'Random Forest': RandomForestClassifier()\n",
    "          ,'MNB':MultinomialNB()\n",
    "          ,'SVC':SVC(kernel='rbf', gamma = 6, random_state=0)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== Logistic Regression ==================================================\n",
      "Train Accuracy:\n",
      " 0.5947060845651426\n",
      "Validation Accuracy:\n",
      " 0.5883161512027492\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        84\n",
      "          1       0.59      0.97      0.74       859\n",
      "          2       0.45      0.04      0.08       485\n",
      "          3       0.00      0.00      0.00        27\n",
      "\n",
      "avg / total       0.50      0.59      0.46      1455\n",
      "\n",
      "Precision Score :  0.5883161512027492\n",
      "Recall Score :  0.5883161512027492\n",
      "F1 score : 0.5883161512027492\n",
      "========================================================================================================================\n",
      "================================================== Decision Tree ==================================================\n",
      "Train Accuracy:\n",
      " 0.7184599518734961\n",
      "Validation Accuracy:\n",
      " 0.540893470790378\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.12      0.08      0.10        84\n",
      "          1       0.61      0.77      0.68       859\n",
      "          2       0.39      0.25      0.30       485\n",
      "          3       0.00      0.00      0.00        27\n",
      "\n",
      "avg / total       0.50      0.54      0.51      1455\n",
      "\n",
      "Precision Score :  0.540893470790378\n",
      "Recall Score :  0.540893470790378\n",
      "F1 score : 0.540893470790378\n",
      "========================================================================================================================\n",
      "================================================== Random Forest ==================================================\n",
      "Train Accuracy:\n",
      " 0.7114128566517703\n",
      "Validation Accuracy:\n",
      " 0.5292096219931272\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.09      0.02      0.04        84\n",
      "          1       0.60      0.74      0.66       859\n",
      "          2       0.36      0.28      0.31       485\n",
      "          3       0.00      0.00      0.00        27\n",
      "\n",
      "avg / total       0.48      0.53      0.50      1455\n",
      "\n",
      "Precision Score :  0.5292096219931272\n",
      "Recall Score :  0.5292096219931272\n",
      "F1 score : 0.5292096219931272\n",
      "========================================================================================================================\n",
      "================================================== MNB ==================================================\n",
      "Train Accuracy:\n",
      " 0.5953936060501891\n",
      "Validation Accuracy:\n",
      " 0.5896907216494846\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        84\n",
      "          1       0.59      0.97      0.74       859\n",
      "          2       0.47      0.06      0.10       485\n",
      "          3       0.00      0.00      0.00        27\n",
      "\n",
      "avg / total       0.51      0.59      0.47      1455\n",
      "\n",
      "Precision Score :  0.5896907216494846\n",
      "Recall Score :  0.5896907216494846\n",
      "F1 score : 0.5896907216494846\n",
      "========================================================================================================================\n",
      "================================================== SVC ==================================================\n",
      "Train Accuracy:\n",
      " 0.6014094190443451\n",
      "Validation Accuracy:\n",
      " 0.5938144329896907\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        84\n",
      "          1       0.59      0.99      0.74       859\n",
      "          2       0.65      0.02      0.04       485\n",
      "          3       0.00      0.00      0.00        27\n",
      "\n",
      "avg / total       0.57      0.59      0.45      1455\n",
      "\n",
      "Precision Score :  0.5938144329896907\n",
      "Recall Score :  0.5938144329896907\n",
      "F1 score : 0.5938144329896907\n",
      "========================================================================================================================\n",
      "Max f1_score is 0.5938144329896907 in model SVC\n"
     ]
    }
   ],
   "source": [
    "f1 = []\n",
    "def run_model(predictors,target, model):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.2, random_state=6)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_scores = model.predict(X_test)\n",
    "    print('Train Accuracy:\\n',model.score(X_train,y_train))\n",
    "    print('Validation Accuracy:\\n',model.score(X_test,y_test))\n",
    "    print('Classification Report:\\n',classification_report(y_test, y_scores))\n",
    "    print(\"Precision Score : \",precision_score(y_test, y_scores,average='micro'))\n",
    "    print(\"Recall Score : \",recall_score(y_test, y_scores,average='micro'))\n",
    "\n",
    "    # calculating the f1 score for the validation set\n",
    "    print(\"F1 score :\", f1_score(y_test, y_scores,average='micro'))\n",
    "    f1.append(f1_score(y_test, y_scores,average='micro'))\n",
    "    print(\"=\"*120)\n",
    "#     a = max(f1)\n",
    "#     b = f1.index(min(f1))\n",
    "    return f1\n",
    "\n",
    "\n",
    "# Predictors\n",
    "X = df_model.drop(['sentiment','clean_text'], axis=1)\n",
    "y = df_model.sentiment\n",
    "models = {'Logistic Regression':LogisticRegression()\n",
    "          ,'Decision Tree':DecisionTreeClassifier()\n",
    "          ,'Random Forest': RandomForestClassifier()\n",
    "          ,'MNB':MultinomialNB()\n",
    "          ,'SVC':SVC()}\n",
    "# models = {'Logistic Regression':LogisticRegression()\n",
    "#           ,'Decision Tree':DecisionTreeClassifier()\n",
    "#           ,'Random Forest': RandomForestClassifier()\n",
    "#           ,'MNB':MultinomialNB()\n",
    "#           ,'SVC':SVC(kernel='linear', gamma = 6, random_state=0)}\n",
    "j = 0\n",
    "for i in models.items():\n",
    "    print('='*50,list(models.keys())[j],'='*50)\n",
    "    model = i[1]\n",
    "    a = run_model(X, y, model)\n",
    "    j=j+1\n",
    "print('Max f1_score is {} in model {}'.format(max(a),list(models.keys())[a.index(max(a))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy:\n",
      " 0.5947060845651426\n",
      "Validation Accuracy:\n",
      " 0.5883161512027492\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        84\n",
      "          1       0.59      0.97      0.74       859\n",
      "          2       0.45      0.04      0.08       485\n",
      "          3       0.00      0.00      0.00        27\n",
      "\n",
      "avg / total       0.50      0.59      0.46      1455\n",
      "\n",
      "Precision Score :  0.5883161512027492\n",
      "Recall Score :  0.5883161512027492\n",
      "F1 score : 0.5883161512027492\n"
     ]
    }
   ],
   "source": [
    "X = df_model.drop(['sentiment','clean_text'], axis=1)\n",
    "y = df_model.sentiment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "y_scores = model.predict(X_test)\n",
    "print('Train Accuracy:\\n',model.score(X_train,y_train))\n",
    "print('Validation Accuracy:\\n',model.score(X_test,y_test))\n",
    "print('Classification Report:\\n',classification_report(y_test, y_scores))\n",
    "print(\"Precision Score : \",precision_score(y_test, y_scores,average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_test, y_scores,average='micro'))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"F1 score :\", f1_score(y_test, y_scores,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1.0, 'decision_function_shape': 'ovo', 'gamma': 4, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grid_search_SVC(dataframe,target):\n",
    "\n",
    "    x_train,x_val,y_train,y_val = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)\n",
    "    model = SVC()\n",
    "    param_grid = { \n",
    "        'C':[1.0,2.0],\n",
    "    'kernel': ['linear', 'rbf'],\n",
    "    'gamma' : [4,5,6],\n",
    "    'decision_function_shape' :['ovo', 'ovr']\n",
    "    }\n",
    "    grid_search_model = GridSearchCV(model, param_grid=param_grid)\n",
    "    grid_search_model.fit(x_train, y_train)\n",
    "    print('Best Parameters are:')\n",
    "    return grid_search_model.best_params_\n",
    "\n",
    "X = df_model.drop(['sentiment','clean_text'], axis=1)\n",
    "y = df_model.sentiment\n",
    "grid_search_SVC(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_model.drop(['sentiment','clean_text'], axis=1)\n",
    "y = df_model.sentiment\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6)\n",
    "model = SVC(C=1.0, kernel='poly', random_state=3)\n",
    "model.fit(X_train, y_train)\n",
    "y_scores = model.predict(X_test)\n",
    "print('Train Accuracy:\\n',model.score(X_train,y_train))\n",
    "print('Validation Accuracy:\\n',model.score(X_test,y_test))\n",
    "print('Classification Report:\\n',classification_report(y_test, y_scores))\n",
    "print(\"Precision Score : \",precision_score(y_test, y_scores,average='micro'))\n",
    "print(\"Recall Score : \",recall_score(y_test, y_scores,average='micro'))\n",
    "\n",
    "# calculating the f1 score for the validation set\n",
    "print(\"F1 score :\", f1_score(y_test, y_scores,average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================================================================================================\n",
      "model: Logistic Regression, columns list ['count_mentions', 'count_hashtags', 'count_capital_words', 'count_excl_quest_marks', 'count_urls', 'count_emojis']\n",
      "========================================================================================================================\n",
      "model: Decision Tree, columns list ['count_words', 'count_mentions', 'count_hashtags', 'count_capital_words', 'count_excl_quest_marks', 'count_urls']\n",
      "========================================================================================================================\n",
      "model: Random Forest, columns list ['count_words', 'count_mentions', 'count_hashtags', 'count_capital_words', 'count_excl_quest_marks', 'count_urls']\n",
      "========================================================================================================================\n",
      "model: MNB, columns list ['count_mentions', 'count_hashtags', 'count_capital_words', 'count_excl_quest_marks', 'count_urls', 'count_emojis']\n",
      "========================================================================================================================\n",
      "model: SVC, columns list ['count_mentions', 'count_hashtags', 'count_capital_words', 'count_excl_quest_marks', 'count_urls', 'count_emojis']\n"
     ]
    }
   ],
   "source": [
    "col = []\n",
    "def sel_feat(X,y, model):\n",
    "    model = model\n",
    "    rfe = RFE(model,6)\n",
    "    rfe = rfe.fit(X,y)\n",
    "    feature_ranking = pd.Series(rfe.ranking_, index=X.columns)\n",
    "    col.append(feature_ranking[feature_ranking.values==1].index.tolist())\n",
    "    print('===='*30)\n",
    "    \n",
    "    return col\n",
    "\n",
    "\n",
    "# Predictors\n",
    "X = df_model.drop(['sentiment','clean_text'], axis=1)\n",
    "y = df_model.sentiment\n",
    " \n",
    "models = {'Logistic Regression':LogisticRegression()\n",
    "          ,'Decision Tree':DecisionTreeClassifier()\n",
    "          ,'Random Forest': RandomForestClassifier()\n",
    "          ,'MNB':MultinomialNB()\n",
    "          ,'SVC':SVC(kernel='linear', gamma = 6, random_state=0)}\n",
    "j = 0\n",
    "for i in models.items():\n",
    "#     print('='*50,list(models.keys())[j],'='*50)\n",
    "    model = i[1]\n",
    "    col = sel_feat(X, y, model)\n",
    "    print('model: {}, columns list {}'.format(list(models.keys())[j],col[j]))\n",
    "    j=j+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================== Logistic Regression ==================================================\n",
      "Train Accuracy:\n",
      " 0.596253007906497\n",
      "Validation Accuracy:\n",
      " 0.5945017182130584\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        84\n",
      "          1       0.59      0.99      0.74       859\n",
      "          2       0.62      0.03      0.06       485\n",
      "          3       0.00      0.00      0.00        27\n",
      "\n",
      "avg / total       0.56      0.59      0.46      1455\n",
      "\n",
      "Precision Score :  0.5945017182130584\n",
      "Recall Score :  0.5945017182130584\n",
      "F1 score : 0.5945017182130584\n",
      "========================================================================================================================\n",
      "================================================== Decision Tree ==================================================\n",
      "Train Accuracy:\n",
      " 0.7181161911309728\n",
      "Validation Accuracy:\n",
      " 0.5395189003436426\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.14      0.10      0.11        84\n",
      "          1       0.61      0.76      0.68       859\n",
      "          2       0.39      0.25      0.30       485\n",
      "          3       0.00      0.00      0.00        27\n",
      "\n",
      "avg / total       0.50      0.54      0.51      1455\n",
      "\n",
      "Precision Score :  0.5395189003436426\n",
      "Recall Score :  0.5395189003436426\n",
      "F1 score : 0.5395189003436426\n",
      "========================================================================================================================\n",
      "================================================== Random Forest ==================================================\n",
      "Train Accuracy:\n",
      " 0.7102096940529391\n",
      "Validation Accuracy:\n",
      " 0.5264604810996564\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.13      0.06      0.08        84\n",
      "          1       0.60      0.74      0.66       859\n",
      "          2       0.35      0.26      0.30       485\n",
      "          3       0.00      0.00      0.00        27\n",
      "\n",
      "avg / total       0.48      0.53      0.50      1455\n",
      "\n",
      "Precision Score :  0.5264604810996564\n",
      "Recall Score :  0.5264604810996564\n",
      "F1 score : 0.5264604810996564\n",
      "========================================================================================================================\n",
      "================================================== MNB ==================================================\n",
      "Train Accuracy:\n",
      " 0.5935029219663115\n",
      "Validation Accuracy:\n",
      " 0.5924398625429553\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        84\n",
      "          1       0.60      0.97      0.74       859\n",
      "          2       0.51      0.06      0.11       485\n",
      "          3       0.00      0.00      0.00        27\n",
      "\n",
      "avg / total       0.52      0.59      0.47      1455\n",
      "\n",
      "Precision Score :  0.5924398625429553\n",
      "Recall Score :  0.5924398625429553\n",
      "F1 score : 0.5924398625429553\n",
      "========================================================================================================================\n",
      "================================================== SVC ==================================================\n",
      "Train Accuracy:\n",
      " 0.5936748023375731\n",
      "Validation Accuracy:\n",
      " 0.5910652920962199\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00        84\n",
      "          1       0.59      1.00      0.74       859\n",
      "          2       0.67      0.00      0.01       485\n",
      "          3       0.00      0.00      0.00        27\n",
      "\n",
      "avg / total       0.57      0.59      0.44      1455\n",
      "\n",
      "Precision Score :  0.5910652920962199\n",
      "Recall Score :  0.5910652920962199\n",
      "F1 score : 0.5910652920962199\n",
      "========================================================================================================================\n",
      "Max f1_score is 0.5945017182130584 in model Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "f1 = []\n",
    "def run_model(predictors,target, model):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.2, random_state=6)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_scores = model.predict(X_test)\n",
    "    print('Train Accuracy:\\n',model.score(X_train,y_train))\n",
    "    print('Validation Accuracy:\\n',model.score(X_test,y_test))\n",
    "    print('Classification Report:\\n',classification_report(y_test, y_scores))\n",
    "    print(\"Precision Score : \",precision_score(y_test, y_scores,average='micro'))\n",
    "    print(\"Recall Score : \",recall_score(y_test, y_scores,average='micro'))\n",
    "\n",
    "    # calculating the f1 score for the validation set\n",
    "    print(\"F1 score :\", f1_score(y_test, y_scores,average='micro'))\n",
    "    f1.append(f1_score(y_test, y_scores,average='micro'))\n",
    "    print(\"=\"*120)\n",
    "#     a = max(f1)\n",
    "#     b = f1.index(min(f1))\n",
    "    return f1\n",
    "\n",
    "\n",
    "\n",
    "models = {'Logistic Regression':LogisticRegression()\n",
    "          ,'Decision Tree':DecisionTreeClassifier()\n",
    "          ,'Random Forest': RandomForestClassifier()\n",
    "          ,'MNB':MultinomialNB()\n",
    "          ,'SVC':SVC(kernel='linear', gamma = 6, random_state=0)}\n",
    "j = 0\n",
    "for i in models.items():\n",
    "    X = df_model[col[j]]\n",
    "    y = df_model.sentiment\n",
    "    print('='*50,list(models.keys())[j],'='*50)\n",
    "    model = i[1]\n",
    "    a = run_model(X, y, model)\n",
    "    j=j+1\n",
    "print('Max f1_score is {} in model {}'.format(max(a),list(models.keys())[a.index(max(a))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnExtractor(TransformerMixin, BaseEstimator):\n",
    "    def __init__(self, cols):\n",
    "        self.cols = cols\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X[self.cols]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "dt = DecisionTreeClassifier()\n",
    "rf = RandomForestClassifier()\n",
    "mnb = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_vect(clf, parameters_clf, X_train, X_test, parameters_text=None, vect=None, is_w2v=False):\n",
    "    \n",
    "    textcountscols = ['count_capital_words','count_emojis','count_excl_quest_marks','count_hashtags'\n",
    "                      ,'count_mentions','count_urls','count_words']\n",
    "    \n",
    "    if is_w2v:\n",
    "        w2vcols = []\n",
    "        for i in range(SIZE):\n",
    "            w2vcols.append(i)\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('w2v', ColumnExtractor(cols=w2vcols))]\n",
    "                                , n_jobs=-1)\n",
    "    else:\n",
    "        features = FeatureUnion([('textcounts', ColumnExtractor(cols=textcountscols))\n",
    "                                 , ('pipe', Pipeline([('cleantext', ColumnExtractor(cols='clean_text')), ('vect', vect)]))]\n",
    "                                , n_jobs=-1)\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('features', features)\n",
    "        , ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    # Join the parameters dictionaries together\n",
    "    parameters = dict()\n",
    "    if parameters_text:\n",
    "        parameters.update(parameters_text)\n",
    "    parameters.update(parameters_clf)\n",
    "    # Make sure you have scikit-learn version 0.19 or higher to use multiple scoring metrics\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5)\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "    print(\"Best CV score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    print(\"Test score with best_estimator_: %0.3f\" % grid_search.best_estimator_.score(X_test, y_test))\n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report Test Data\")\n",
    "    print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))\n",
    "                        \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid settings for the vectorizers (Count and TFIDF)\n",
    "parameters_vect = {\n",
    "    'features__pipe__vect__max_df': (0.25, 0.5, 0.75),\n",
    "    'features__pipe__vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'features__pipe__vect__min_df': (1,2)\n",
    "}\n",
    "# Parameter grid settings for MultinomialNB\n",
    "parameters_mnb = {\n",
    "    'clf__alpha': (0.25, 0.5, 0.75)\n",
    "}\n",
    "# Parameter grid settings for LogisticRegression\n",
    "parameters_logreg = {\n",
    "    'clf__C': (0.25, 0.5, 1.0),\n",
    "    'clf__penalty': ('l1', 'l2')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvect = CountVectorizer()\n",
    "# MultinomialNB\n",
    "best_mnb_countvect = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=countvect)\n",
    "joblib.dump(best_mnb_countvect, 'data/best_mnb_countvect.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "best_logreg_countvect = grid_vect(logreg, parameters_logreg, X_train, X_test, parameters_text=parameters_vect, vect=countvect)\n",
    "joblib.dump(best_logreg_countvect, 'data/best_logreg_countvect.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvect = TfidfVectorizer()\n",
    "# MultinomialNB\n",
    "best_mnb_tfidf = grid_vect(mnb, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)\n",
    "joblib.dump(best_mnb_tfidf, 'datat/best_mnb_tfidf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "best_logreg_tfidf = grid_vect(logreg, parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=tfidfvect)\n",
    "joblib.dump(best_logreg_tfidf, 'data/best_logreg_tfidf.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 50\n",
    "X_train['clean_text_wordlist'] = X_train.clean_text.apply(lambda x : word_tokenize(x))\n",
    "X_test['clean_text_wordlist'] = X_test.clean_text.apply(lambda x : word_tokenize(x))\n",
    "model = gensim.models.Word2Vec(X_train.clean_text_wordlist, min_count=1, size=SIZE, window=5, workers=4)\n",
    "model.most_similar('plane', topn=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_w2v_vector(w2v_dict, tweet):\n",
    "    list_of_word_vectors = [w2v_dict[w] for w in tweet if w in w2v_dict.vocab.keys()]\n",
    "    \n",
    "    if len(list_of_word_vectors) == 0:\n",
    "        result = [0.0]*SIZE\n",
    "    else:\n",
    "        result = np.sum(list_of_word_vectors, axis=0) / len(list_of_word_vectors)\n",
    "        \n",
    "    return result\n",
    "X_train_w2v = X_train['clean_text_wordlist'].apply(lambda x: compute_avg_w2v_vector(model.wv, x))\n",
    "X_test_w2v = X_test['clean_text_wordlist'].apply(lambda x: compute_avg_w2v_vector(model.wv, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2v = pd.DataFrame(X_train_w2v.values.tolist(), index= X_train.index)\n",
    "X_test_w2v = pd.DataFrame(X_test_w2v.values.tolist(), index= X_test.index)\n",
    "# Concatenate with the TextCounts variables\n",
    "X_train_w2v = pd.concat([X_train_w2v, X_train.drop(['clean_text', 'clean_text_wordlist'], axis=1)], axis=1)\n",
    "X_test_w2v = pd.concat([X_test_w2v, X_test.drop(['clean_text', 'clean_text_wordlist'], axis=1)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logreg_w2v = grid_vect(logreg, parameters_logreg, X_train_w2v, X_test_w2v, is_w2v=True)\n",
    "joblib.dump(best_logreg_w2v, 'data/best_logreg_w2v.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
