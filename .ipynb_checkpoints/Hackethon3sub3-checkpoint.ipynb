{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import emoji\n",
    "from pprint import pprint\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "from sklearn.metrics import roc_auc_score ,mean_squared_error,accuracy_score,classification_report,confusion_matrix,roc_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.externals import joblib\n",
    "import gensim\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Loading the Train data\n",
    "def load_train():\n",
    "    df = pd.read_csv('data/train.csv')\n",
    "    df = df[['tweet', 'sentiment']]\n",
    "    df = df.dropna()\n",
    "#     print(df.info())\n",
    "    return df\n",
    "\n",
    "# df = load_train()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Loading the Train data\n",
    "def load_test():\n",
    "    df = pd.read_csv('data/test.csv')\n",
    "    df = df[['tweet_id','tweet']]\n",
    "#     print(df.info())\n",
    "    return df\n",
    "\n",
    "# dftest = load_test()\n",
    "# df1 = dftest[['tweet']]\n",
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCounts(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def count_regex(self, pattern, tweet):\n",
    "        return len(re.findall(pattern, tweet))\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        # fit method is used when specific operations need to be done on the train data, but not on the test data\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        count_words = X.apply(lambda x: self.count_regex(r'\\w+', str(x))) \n",
    "        count_mentions = X.apply(lambda x: self.count_regex(r'@\\w+', str(x)))\n",
    "        count_hashtags = X.apply(lambda x: self.count_regex(r'#\\w+', str(x)))\n",
    "        count_capital_words = X.apply(lambda x: self.count_regex(r'\\b[A-Z]{2,}\\b', str(x)))\n",
    "        count_excl_quest_marks = X.apply(lambda x: self.count_regex(r'!|\\?', str(x)))\n",
    "        count_urls = X.apply(lambda x: self.count_regex(r'http.?://[^\\s]+[\\s]?', str(x)))\n",
    "        # We will replace the emoji symbols with a description, which makes using a regex for counting easier\n",
    "        # Moreover, it will result in having more words in the tweet\n",
    "        count_emojis = X.apply(lambda x: emoji.demojize(str(x))).apply(lambda x: self.count_regex(r':[a-z_&]+:', str(x)))\n",
    "        \n",
    "        df = pd.DataFrame({'count_words': count_words\n",
    "                           , 'count_mentions': count_mentions\n",
    "                           , 'count_hashtags': count_hashtags\n",
    "                           , 'count_capital_words': count_capital_words\n",
    "                           , 'count_excl_quest_marks': count_excl_quest_marks\n",
    "                           , 'count_urls': count_urls\n",
    "                           , 'count_emojis': count_emojis\n",
    "                          })\n",
    "        return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CleanText(BaseEstimator, TransformerMixin):\n",
    "    def remove_mentions(self, input_text):\n",
    "        return re.sub(r'@\\w+', '', str(input_text))\n",
    "    \n",
    "    def remove_urls(self, input_text):\n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', str(input_text))\n",
    "    \n",
    "    def emoji_oneword(self, input_text):\n",
    "        # By compressing the underscore, the emoji is kept as one word\n",
    "        return input_text.replace('_','')\n",
    "    \n",
    "    def remove_punctuation(self, input_text):\n",
    "        # Make translation table\n",
    "        punct = string.punctuation\n",
    "        trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "        return input_text.translate(trantab)\n",
    "    def remove_digits(self, input_text):\n",
    "        return re.sub('\\d+', '', str(input_text))\n",
    "    \n",
    "    def to_lower(self, input_text):\n",
    "        return input_text.lower()\n",
    "    \n",
    "    def remove_stopwords(self, input_text):\n",
    "        stopwords_list = stopwords.words('english')\n",
    "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "        whitelist = [\"n't\", \"not\", \"no\"]\n",
    "        words = input_text.split() \n",
    "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "        return \" \".join(clean_words) \n",
    "    \n",
    "    def stemming(self, input_text):\n",
    "        porter = PorterStemmer()\n",
    "        words = input_text.split() \n",
    "        stemmed_words = [porter.stem(word) for word in words]\n",
    "        return \" \".join(stemmed_words)\n",
    "    \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.emoji_oneword).apply(self.remove_punctuation).apply(self.remove_digits).apply(self.to_lower).apply(self.remove_stopwords).apply(self.stemming)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score is: 0.6701030927835051\n",
      "1    1420\n",
      "2    391 \n",
      "0    8   \n",
      "Name: sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = load_train()  # load train data\n",
    "df1 = load_test() # load test data\n",
    "\n",
    "# create numerical feature for train\n",
    "tc = TextCounts()\n",
    "df_train = tc.fit_transform(df.tweet)\n",
    "df_train['sentiment'] = df.sentiment\n",
    "# create numerical feature for test\n",
    "df_test = tc.fit_transform(df1['tweet'])\n",
    "# clean text for train and CountVectorizer train data\n",
    "ct = CleanText()\n",
    "train_clean = ct.fit_transform(df['tweet'])\n",
    "# clean text for test and CountVectorizer test data\n",
    "ct = CleanText()\n",
    "test_clean = ct.fit_transform(df1['tweet'])\n",
    "\n",
    "df_model = df_train\n",
    "df_model['clean_text'] = train_clean\n",
    "df1_model = df_test\n",
    "df1_model['clean_text'] = test_clean\n",
    "\n",
    "X = df_model['clean_text']\n",
    "X1 = df1_model['clean_text']\n",
    "y = df_model.sentiment\n",
    "tfidf = TfidfVectorizer(stop_words = 'english')\n",
    "Xt = tfidf.fit_transform(X)\n",
    "df_test = df1_model['clean_text']\n",
    "df_test1 = tfidf.transform(df_test)\n",
    "\n",
    "def LogReg(X,y):\n",
    "    global model\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_score = model.predict(X_test)\n",
    "#     print('Train Accuracy:\\n',model.score(X_train,y_train))\n",
    "#     print('Validation Accuracy:\\n',model.score(X_test,y_test))\n",
    "#     print('Classification Report:\\n',classification_report(y_test, y_score))\n",
    "#     print(\"Precision Score : \",precision_score(y_test, y_score,average='micro'))\n",
    "#     print(\"Recall Score : \",recall_score(y_test, y_score,average='micro'))\n",
    "    # calculating the f1 score for the validation set\n",
    "    f1 = f1_score(y_test, y_score,average='micro')\n",
    "    \n",
    "    return f1\n",
    "\n",
    "#trainning\n",
    "X = Xt\n",
    "y = df_model.sentiment\n",
    "f1 = LogReg(X,y)    \n",
    "print('f1 score is:',f1)\n",
    "\n",
    "#testing function\n",
    "def prediction(test):\n",
    "    y_pred = model.predict(test)\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "\n",
    "# Storing the Id column\n",
    "Id = dftest[['tweet_id']]\n",
    "\n",
    "#predicting on test file\n",
    "y_pred = pd.DataFrame(prediction(df_test1),columns=['sentiment']) \n",
    "print(y_pred['sentiment'].value_counts())\n",
    "submission = pd.concat([Id,y_pred['sentiment']],1)\n",
    "submission.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
